# Ralph Progress Log - Performance Fixes
Started: 2026-01-10
Branch: ralph/perf-fixes

---

## Codebase Patterns (READ FIRST!)

### Channel Selection Guide
| Data Type | Channel | Reason |
|-----------|---------|--------|
| Events (MQTT/ACC) | mpsc + try_send | High volume, drops acceptable |
| Door state (RS485) | watch | Stateful, latest value matters, lossless |
| Gate commands | mpsc to worker | Decouples from tracker, worker owns network I/O |
| Journey egress | mpsc to writer | Decouples from tracker, writer owns file I/O |

### Non-Blocking Pattern
```rust
// BAD: blocks caller
event_tx.send(event).await?;

// GOOD: returns immediately, tracks drops
match event_tx.try_send(event) {
    Ok(()) => {},
    Err(TrySendError::Full(_)) => {
        metrics.events_dropped.fetch_add(1, Ordering::Relaxed);
        if last_warn.elapsed() > Duration::from_secs(1) {
            warn!("event dropped: channel full");
            last_warn = Instant::now();
        }
    }
    Err(TrySendError::Disconnected(_)) => return,
}
```

### Watch Channel for Stateful Data
```rust
// RS485 door state - latest value always available
let (door_tx, door_rx) = watch::channel(DoorStatus::Unknown);

// Sender (RS485 monitor): never blocks, never fails
door_tx.send_replace(new_status);

// Receiver (Tracker): get latest, never blocks
let status = *door_rx.borrow();
```

### Worker Task Pattern
```rust
// Create channel and spawn worker
let (cmd_tx, cmd_rx) = mpsc::channel(64);
let worker = GateCmdWorker::new(gate_controller, cmd_rx, metrics);
tokio::spawn(async move { worker.run().await });

// In tracker: enqueue and return immediately
cmd_tx.try_send(GateCmd { track_id, received_at }).ok();
```

### Metrics Patterns
- Use `AtomicU64` for lock-free counters
- Add per-source drop counters: `mqtt_events_dropped`, `acc_events_dropped`, `gate_cmds_dropped`
- Queue depth: sample channel len() periodically or wrap in Arc
- Latency: record timestamp at enqueue, measure delta at processing

### Key Files
| File | Purpose | Changes Needed |
|------|---------|----------------|
| `src/services/tracker/handlers.rs:770` | Gate command logic | Enqueue instead of await |
| `src/services/tracker/mod.rs:83` | Main event loop | Add watch receiver for door state |
| `src/services/gate_worker.rs` (NEW) | Gate command worker | Owns GateController, measures latency |
| `src/io/mqtt.rs:60` | MQTT client | try_send + drop counter |
| `src/io/acc_listener.rs` | ACC listener | try_send + drop counter |
| `src/io/rs485.rs` | Door state monitor | watch channel instead of mpsc |
| `src/io/cloudplus.rs:579` | TCP gate queue | try_send with explicit error |
| `src/io/egress.rs` | Journey writer | Async worker, spawn_blocking |
| `src/infra/metrics.rs` | Metrics | Add drop counters, queue depth |

### Problem Context
The tracker loop blocks on network I/O:
1. `send_gate_open_command` awaits TCP (up to 5s timeout)
2. MQTT/ACC use `send().await` (blocks if tracker slow)
3. RS485 uses mpsc + try_send (drops door state on full channel)

Consequences:
- Gate commands arrive late ("gate open too late")
- MQTT eventloop stalls → missed keepalives → disconnects
- Door state can be lost, breaking correlation

### Solution Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                        INGRESS                               │
├─────────────────────────────────────────────────────────────┤
│  MQTT ──try_send──> │                                        │
│  ACC  ──try_send──> │  event_rx (mpsc, 1000 capacity)        │
│                     │                                        │
│  RS485 ──watch───> door_rx (watch, always latest)            │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                       TRACKER                                │
│  - Receives events from event_rx                            │
│  - Reads door state from door_rx.borrow()                   │
│  - Enqueues gate commands to gate_cmd_tx (never awaits)     │
│  - Enqueues journeys to egress_tx (never awaits)            │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                       WORKERS                                │
├─────────────────────────────────────────────────────────────┤
│  GateCmdWorker: gate_cmd_rx → gate.send_open_command()      │
│    - Measures queue delay + send latency                    │
│    - Updates metrics                                        │
│                                                             │
│  EgressWriter: egress_rx → spawn_blocking file write        │
│    - Batches journeys                                       │
│    - Flushes on count or timer                              │
└─────────────────────────────────────────────────────────────┘
```

---

## Completed Stories

### US-001: Decouple gate commands from tracker loop ✓
**Commit:** 0965cc3 feat: [US-001] - Decouple gate commands from tracker loop

**Changes:**
- Created `src/services/gate_worker.rs` with `GateCmdWorker` task
- Worker owns `GateController` and receives commands via mpsc channel
- Tracker enqueues gate commands with `try_send()` - never awaits network I/O
- Worker measures queue delay and send latency, logs timing breakdown
- All event handlers made synchronous (no more `.await` on hot path)
- Gate state payload changed from "cmd_sent" to "cmd_enqueued"

**Files modified:**
- `src/services/gate_worker.rs` (NEW)
- `src/services/tracker/handlers.rs` - sync handlers, try_send to channel
- `src/services/tracker/mod.rs` - uses gate_cmd_tx instead of gate Arc
- `src/services/mod.rs` - exports gate_worker module
- `src/main.rs` - spawns GateCmdWorker task
- `src/services/tracker/tests.rs` - updated for sync process_event

**Key insight:** This is the PRIMARY fix for "gate open too late" - network I/O
no longer blocks the tracker event loop.

### US-002: Make MQTT/ACC event sending non-blocking with drop metrics ✓
**Commit:** 9d0604d feat: [US-002] - Make MQTT/ACC event sending non-blocking with drop metrics

**Changes:**
- MQTT `start_mqtt_client` now takes `Arc<Metrics>` and uses `try_send()`
- ACC `start_acc_listener` now takes `Arc<Metrics>` and uses `try_send()`
- On `TrySendError::Full`, increment counter and log rate-limited warning (1/sec)
- Added `mqtt_events_dropped` and `acc_events_dropped` counters to Metrics
- Updated main.rs to pass metrics to both MQTT and ACC

**Files modified:**
- `src/io/mqtt.rs` - try_send with drop metrics
- `src/io/acc_listener.rs` - try_send with drop metrics
- `src/infra/metrics.rs` - added drop counters
- `src/main.rs` - pass metrics to MQTT and ACC

**Key insight:** MQTT eventloop must never block - stalls cause keepalive
failures and disconnects.

### US-003: RS485 door state: use watch channel for lossless delivery ✓
**Commit:** 404785a feat: [US-003] - RS485 door state: use watch channel for lossless delivery

**Changes:**
- RS485Monitor now uses `watch::Sender<DoorStatus>` instead of `mpsc::Sender<ParsedEvent>`
- Door state sent via `send_replace()` - never blocks, never fails, overwrites previous
- Tracker receives door state via `watch::Receiver` in select! loop
- Uses `changed()` + `borrow()` pattern to detect and process state changes
- Removed DoorStateChange from event channel path (still in EventType for exhaustive match)
- Reduced MAX_READ_ATTEMPTS from 50 to 10 to cap RS485 stall time at ~500ms

**Files modified:**
- `src/io/rs485.rs` - watch channel, reduced MAX_READ_ATTEMPTS
- `src/main.rs` - create watch channel, pass door_rx to Tracker
- `src/services/tracker/mod.rs` - accept door_rx, poll in run() loop
- `src/services/tracker/tests.rs` - create test door_rx for Tracker::new
- `src/bin/tui.rs` - fixed pre-existing unused variable warning

**Key insight:** Door state is stateful data - latest value is what matters,
not the event stream. Watch channel guarantees lossless delivery and the
latest value is always available via `borrow()`.

### US-004: CloudPlus gate queue: try_send with explicit failure handling ✓
**Commit:** ffd021f feat: [US-004] - CloudPlus gate queue: try_send with explicit failure handling

**Changes:**
- Changed `send_open()` in cloudplus.rs from `send().await` to `try_send()`
- On `TrySendError::Full`, return `Err("queue full")` instead of blocking
- On `TrySendError::Closed`, log error and return `Err("channel closed")`
- `send_open()` is no longer async (try_send is synchronous)
- In gate.rs `send_open_tcp()`, log WARN when queue full with track_id context
- Increment `gate_cmds_dropped` metric on queue full
- GateController::new() now takes optional `Arc<Metrics>` for drop tracking
- Reordered main.rs to create metrics before GateController

**Files modified:**
- `src/io/cloudplus.rs` - try_send with explicit error handling
- `src/services/gate.rs` - WARN logging and metrics increment
- `src/main.rs` - pass metrics to GateController

**Key insight:** A dropped gate command means the customer waits - this is
acceptable if TCP is so backed up, as dropping is better than blocking
the entire gate worker indefinitely. The metric makes this visible.

---

### US-005: Add queue depth and lag metrics for diagnosability ✓
**Commit:** 329455a feat: [US-005] - Add queue depth and lag metrics for diagnosability

**Changes:**
- Added `gate_queue_delay_buckets` histogram to track time from enqueue to worker pickup
- Added `event_queue_depth` and `gate_queue_depth` gauges (sampled periodically)
- Gate worker now records queue delay to metrics via `record_gate_queue_delay()`
- Main.rs metrics reporter samples queue depths using `max_capacity - capacity`
- All new metrics exposed via Prometheus endpoint:
  - `gateway_gate_queue_delay_us_bucket` (histogram)
  - `gateway_gate_queue_delay_p99_us`, `gateway_gate_queue_delay_max_us` (gauges)
  - `gateway_event_queue_depth`, `gateway_gate_queue_depth` (gauges)
- Extracted `update_atomic_max()` helper to deduplicate CAS pattern

**Files modified:**
- `src/infra/metrics.rs` - new metrics, helper function
- `src/services/gate_worker.rs` - record queue delay
- `src/io/prometheus.rs` - expose new metrics
- `src/main.rs` - queue depth sampling

**Key insight:** Queue depth metrics allow diagnosing "late gate open" issues by
observing backlog buildup. Queue delay histogram shows how long commands wait
before being processed by the worker.

---

### US-006: Move journey egress to async worker ✓
**Commit:** 3851dd2 feat: [US-006] - Move journey egress to async worker

**Changes:**
- Created `EgressWriter` async task in `src/io/egress.rs`
- Worker receives journeys via `mpsc::Receiver<Journey>` from tracker
- Batches journeys and flushes on count (10) or timer (1 second)
- Uses `spawn_blocking` for file I/O to avoid blocking async runtime
- Uses `BufWriter` for efficient batch writes
- Tracker now uses `journey_tx.try_send()` instead of direct file writes
- Removed `Egress` struct from Tracker (kept for unit tests)
- Updated main.rs to spawn EgressWriter task

**Files modified:**
- `src/io/egress.rs` - added EgressWriter, create_egress_writer, write_journeys_to_file
- `src/io/mod.rs` - re-export EgressWriter and create_egress_writer
- `src/main.rs` - spawn EgressWriter, pass journey_tx to Tracker
- `src/services/tracker/mod.rs` - use journey_tx channel instead of Egress
- `src/services/tracker/tests.rs` - add journey_tx channel to test tracker

**Key insight:** File I/O, even on SSD, can block the async runtime. Using
`spawn_blocking` ensures file writes don't stall the tracker event loop.
Batching reduces syscall overhead.

---

### US-007: Cache zone names to avoid per-event allocation ✓
**Commit:** 33ade6e feat: [US-007] - Cache zone names to avoid per-event allocation

**Changes:**
- Changed `zone_names` HashMap from `HashMap<i32, String>` to `HashMap<i32, Arc<str>>`
- Updated `zone_name()` method to return `Arc<str>` instead of `String`
- Arc<str> cloning is cheap (just atomic ref count increment) vs String allocation
- Unknown zones still allocate via `Arc::from(format!(...))` - unavoidable
- Updated callers in handlers.rs to use `.to_string()` at serialization boundaries

**Files modified:**
- `src/infra/config.rs` - Arc<str> storage and return type
- `src/services/tracker/handlers.rs` - updated callers, added Arc import

**Key insight:** The benefit is that repeated lookups of known zones (POS_1, GATE_1, etc.)
no longer allocate. The `.to_string()` calls at serialization boundaries are necessary
because payload types use `Option<String>`, but these allocations only happen once per
event at the serialization point, not on every config lookup.

---

### US-008: Reduce per-event logging or add sampling ✓
**Commit:** acbe170 feat: [US-008] - Reduce per-event logging or add sampling

**Changes:**
- Demoted 8 high-frequency diagnostic logs from `info!` to `debug!` level:
  - `track_stitched`: fires on every track stitch event
  - `track_pending_stitch`: fires on every track delete
  - `entry_line_backward_returning_to_store`: fires on line cross
  - `journey_returned_backward_entry`: journey outcome classification
  - `journey_exit_inferred`: journey outcome classification
  - `journey_returned_pos_zone`: journey outcome classification
  - `journey_returned_store_zone`: journey outcome classification
  - `journey_lost`: journey outcome classification

**Logs kept at info! level (critical events):**
- `gate_entry_not_authorized`: gate blocked diagnostic
- `journey_complete`: exit line crossed milestone
- `door_state_change`: RS485 state change
- `acc_group_authorized` / `acc_unmatched`: business events
- `late_acc_after_gate_entry`: timing diagnostic

**Files modified:**
- `src/services/tracker/handlers.rs` - 8 info! → debug! demotions

**Key insight:** Zone entry/exit logs were already at debug! level (lines 268, 349).
The demoted logs were high-frequency track lifecycle and journey outcome classification
events that add noise without helping diagnose production issues.

---

### US-009: Remove MQTT parse_frame HashMap allocation ✓
**Commit:** 2c45a0e feat: [US-009] - Remove MQTT parse_frame HashMap allocation

**Changes:**
- Removed `HashMap<i64, [f64; 3]>` allocation per MQTT frame
- Replaced with linear search using `iter().find().filter().map()`
- Position lookup now scans `tracked_objects` array directly

**Files modified:**
- `src/io/mqtt.rs` - replaced HashMap with linear search in `parse_frame`

**Key insight:** For small collections (<10 tracked objects per frame), linear
search avoids HashMap allocation overhead and has better cache locality. The
O(events * tracked_objects) complexity is acceptable given both are small.

---

## Story Progress

